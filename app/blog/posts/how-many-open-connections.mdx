---
title: Handling Many Connected Users: How Apps Scale
publishedAt: "2025-03-04"
tags: Scaling, WebSockets, Load Balancing, Horizontal Scaling, Chat Apps, AWS
summary: Building apps that need to maintain constant connections with many users (like chat or real-time games) requires careful planning. This article explains how a single server has limitations on the number of simultaneous connections it can handle due to operating system constraints (like file descriptors). It then details how techniques like load balancing (distributing connections across multiple servers) and horizontal scaling (adding more servers) are used to overcome these limits and support millions of concurrent users.
---

# Handling Many Connected Users: How Apps Scale

Building applications that require persistent connections with a large number of users, such as chat platforms or real-time multiplayer games, presents significant scaling challenges. A single server has inherent limitations in the number of simultaneous connections it can manage. This article delves into these limitations and explores the architectural patterns used to overcome them, enabling applications to support millions of concurrent users.

## 1. Connection Limits and Your Server

When your app has a user connected (especially with technologies like **WebSockets**), the server needs to keep track of that connection. The operating system does this using something called a **file descriptor (FD)**.

Think of **FDs as tickets**: Each open connection gets a ticket. Your server has a maximum number of tickets it can hand out at once (the **file descriptor limit**).

**Default Limits:** Operating systems often have low default limits (e.g., around 1000 connections). While these limits can be increased, the server's memory and processing power will eventually become bottlenecks.

## 2. Why Real-Time Apps Need Many Connections

Unlike regular web browsing (short requests), real-time apps like chat keep connections open using protocols like **WebSockets**. This allows instant two-way communication.

This "always-on" nature means each connected user constantly occupies a file descriptor and consumes server resources.

## 3. Spreading the Load: Load Balancing

To handle more concurrent connections than a single server allows, you use a **load balancer**.

Think of a **load balancer as a smart traffic director**: It sits in front of your servers and distributes incoming connection requests evenly across multiple backend servers. This prevents any single server from being overwhelmed.

Load balancers can operate at different network layers (**Layer 4** for basic IP/port routing, **Layer 7** for more intelligent routing based on application data like WebSocket handshakes).

## 4. Adding More Power: Horizontal Scaling

Even with load balancing, each individual server has its capacity limits. To scale to millions of users, you employ **horizontal scaling**.

Think of **horizontal scaling as adding more identical servers**: When the existing servers start getting busy, you spin up more servers, and the load balancer automatically starts distributing new connections to them.

## 5. Managing Data Flow: Databases and Message Queues

Besides managing connections, real-time apps need to efficiently handle the flow of data (messages, events).

**Databases** store persistent data like user accounts and message history. They need to be scalable to handle large volumes of data.

**Message Queues** (like **Kafka** or **Redis Pub/Sub**) are used for asynchronous communication between different parts of the system, ensuring reliable and real-time delivery of messages to connected clients across multiple servers.

## 6. How Big Players Scale

Large-scale applications like WhatsApp and Discord utilize these techniques extensively:

- Running numerous server instances.
- Employing sophisticated load balancing strategies.
- Using scalable databases and message queues.
- Often leveraging cloud infrastructure (like **AWS** or **GCP**) for dynamic scaling and global distribution.

**In essence:** To support a massive number of concurrent, persistent connections, modern applications don't rely on a single powerful server. Instead, they use a distributed architecture involving load balancers to spread the initial connection load and horizontal scaling to increase the overall capacity by adding more server instances. Efficient data management through scalable databases and message queues is also crucial for the real-time functionality of these applications.
